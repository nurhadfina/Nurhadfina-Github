{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nurhadfina/Nurhadfina-Github/blob/main/CPC353_ASSIGNMENT2_G32.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ASSIGNMENT 2: Natural Language Processing Semester I 2025/2026\n",
        "Title: Stock trend prediction with news sentiment\n",
        "\n",
        "Objectives\n",
        "\n",
        "O1: Construct natural language processing and deep learning components in problems\n",
        "involving prediction, classification and sequence modeling in text and speech.\n",
        "\n",
        "O2: Design solutions using natural language processing and deep learning techniques for\n",
        "problems in text and speech analytics."
      ],
      "metadata": {
        "id": "B45cqwt4OjQt"
      },
      "id": "B45cqwt4OjQt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "  TASK 1 : Data Loading, Labeling, and Splitting"
      ],
      "metadata": {
        "id": "tWBLWktlK_6R"
      },
      "id": "tWBLWktlK_6R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7423aea4-30f6-4f2c-9895-d5a564ca402c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7423aea4-30f6-4f2c-9895-d5a564ca402c",
        "outputId": "2ec2d13f-7540-486d-a051-edd1b71f4d74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Dataset Loaded \n",
            "                                               Title  \\\n",
            "0  100 startups participate in Maxis' Market Acce...   \n",
            "1      16.89% stake in Subur Tiasa traded off-market   \n",
            "2  Najib wanted 1MDB's Genting Sanyen deal sped u...   \n",
            "3        25bps OPR cut likely in 2H20, says Manulife   \n",
            "4  A 25-month extension on concession pushes Phar...   \n",
            "5             3.7% of Yong Tai transacted off-market   \n",
            "6              3A, Ruberex, Thriven, Kanger, UniWall   \n",
            "7      40% stake in IWH-CREC may cost Ekovest RM1.5b   \n",
            "8      4.41% Kronologi Asia shares traded off-market   \n",
            "9                    4.96% of MMAG traded off market   \n",
            "\n",
            "                        Time     Name  Quote  Before  After  \n",
            "0  2019-12-12T23:50:12+08:00    MAXIS   6012   5.160  5.110  \n",
            "1  2020-02-20T22:41:12+08:00    SUBUR   6904   0.610  0.610  \n",
            "2  2020-07-16T17:42:30+08:00  GENTING   3182   4.080  4.060  \n",
            "3  2020-01-16T17:03:43+08:00  MANULFE   1058   2.420  2.420  \n",
            "4  2019-11-11T10:49:58+08:00   PHARMA   7081   2.620  2.430  \n",
            "5  2019-09-05T19:55:54+08:00  YONGTAI   7066   0.195  0.200  \n",
            "6  2020-03-04T13:23:15+08:00   KANGER    170   0.125  0.120  \n",
            "7  2020-09-16T10:00:00+08:00  EKOVEST   8877   0.610  0.580  \n",
            "8  2020-02-12T23:54:51+08:00    KRONO    176   0.895  0.890  \n",
            "9  2019-07-23T22:42:17+08:00     MMAG     34   0.245  0.235  \n",
            "\n",
            "\n",
            "Step 2: Data Labeled (First 10 lines)\n",
            "      Name  Before  After  Relative_Change Label\n",
            "0    MAXIS   5.160  5.110        -0.968992  flat\n",
            "1    SUBUR   0.610  0.610         0.000000  flat\n",
            "2  GENTING   4.080  4.060        -0.490196  flat\n",
            "3  MANULFE   2.420  2.420         0.000000  flat\n",
            "4   PHARMA   2.620  2.430        -7.251908  flat\n",
            "5  YONGTAI   0.195  0.200         2.564103  flat\n",
            "6   KANGER   0.125  0.120        -4.000000  flat\n",
            "7  EKOVEST   0.610  0.580        -4.918033  flat\n",
            "8    KRONO   0.895  0.890        -0.558659  flat\n",
            "9     MMAG   0.245  0.235        -4.081633  flat\n",
            "\n",
            "\n",
            "Step 3: Data Split\n",
            "Total Records: 24388\n",
            "Training Set: 17071 records (70%)\n",
            "Validation Set: 4877 records (20%)\n",
            "Test Set: 2440 records (10%)\n",
            "Training Data Sample (First 10 lines)\n",
            "      Name Label\n",
            "0    MAXIS  flat\n",
            "1    SUBUR  flat\n",
            "2  GENTING  flat\n",
            "3  MANULFE  flat\n",
            "4   PHARMA  flat\n",
            "5  YONGTAI  flat\n",
            "6   KANGER  flat\n",
            "7  EKOVEST  flat\n",
            "8    KRONO  flat\n",
            "9     MMAG  flat\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Load the dataset\n",
        "df = pd.read_csv('stock_trend.csv')\n",
        "\n",
        "print(\"Step 1: Dataset Loaded \")\n",
        "print(df.head(10))\n",
        "print(\"\\n\")\n",
        "\n",
        "# 2. Labeling the Data\n",
        "# Calculate Relative Change (%) = ((After - Before) / Before) * 100\n",
        "df['Relative_Change'] = ((df['After'] - df['Before']) / df['Before']) * 100\n",
        "\n",
        "# Function to define trend based on relative change\n",
        "def define_trend(change):\n",
        "    if change > 10:\n",
        "        return 'uptrend'\n",
        "    elif change < -10:\n",
        "        return 'downtrend'\n",
        "    else:\n",
        "        return 'flat'\n",
        "\n",
        "# Apply the function to create the 'Label' column\n",
        "df['Label'] = df['Relative_Change'].apply(define_trend)\n",
        "\n",
        "print(\"Step 2: Data Labeled (First 10 lines)\")\n",
        "# Displaying relevant columns to verify the logic\n",
        "print(df[['Name', 'Before', 'After', 'Relative_Change', 'Label']].head(10))\n",
        "print(\"\\n\")\n",
        "\n",
        "# 3. Splitting the Data\n",
        "# 70% Training, 20% Validation, 10% Test\n",
        "train_size = int(0.7 * len(df))\n",
        "val_size = int(0.2 * len(df))\n",
        "\n",
        "# Slicing the dataframe\n",
        "train_data = df.iloc[:train_size]\n",
        "val_data = df.iloc[train_size : train_size + val_size]\n",
        "test_data = df.iloc[train_size + val_size:]\n",
        "\n",
        "print(f\"Step 3: Data Split\")\n",
        "print(f\"Total Records: {len(df)}\")\n",
        "print(f\"Training Set: {len(train_data)} records (70%)\")\n",
        "print(f\"Validation Set: {len(val_data)} records (20%)\")\n",
        "print(f\"Test Set: {len(test_data)} records (10%)\")\n",
        "\n",
        "print(\"Training Data Sample (First 10 lines)\")\n",
        "print(train_data[['Name', 'Label']].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK 2"
      ],
      "metadata": {
        "id": "pGunePMxL_XA"
      },
      "id": "pGunePMxL_XA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading GloVe Word Embeddings"
      ],
      "metadata": {
        "id": "wcNjB_RbNh7D"
      },
      "id": "wcNjB_RbNh7D"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1179d3dd-bc54-451f-8f5f-b4d3fbdb4190",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1179d3dd-bc54-451f-8f5f-b4d3fbdb4190",
        "outputId": "0d45667a-d350-449f-fbad-ee0976a5487a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 73261 word vectors.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Download the GloVe embedding file if not already present\n",
        "glove_path = 'glove.6B.100d.txt'\n",
        "if not os.path.exists(glove_path):\n",
        "   print(\"Downloading GloVe embeddings...\")\n",
        "   !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "   !unzip glove.6B.zip\n",
        "   print(\"GloVe embeddings downloaded.\")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(glove_path, encoding='utf8') as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Preprocessing & Tokenization\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "aJTHg8orNjgq"
      },
      "id": "aJTHg8orNjgq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM Model Architecture"
      ],
      "metadata": {
        "id": "jU0g1t2pN_BT"
      },
      "id": "jU0g1t2pN_BT"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Flatten, Embedding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "sent_length = 100   # Adjust based on headline lengths\n",
        "n_features = 100    # Because use glove.6B.100d.txt\n",
        "n_output = 3        # Three classes: uptrend, flat, downtrend\n",
        "\n",
        "# Define vocab_size before its first use\n",
        "vocab_size = 10000 # Example value, adjust as needed based on your dataset's vocabulary or desired maximum\n",
        "\n",
        "# --- GLOVE EMBEDDING & OOV HANDLING ---\n",
        "train_data['Title'] = train_data['Title'].str.lower().str.replace(r'[^A-Za-z0-9 ]','') # Fix SyntaxWarning and remove all non-alphanumeric and non-space characters\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_data['Title'])\n",
        "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding/OOV\n",
        "embedding_matrix = np.zeros((vocab_size, n_features))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < vocab_size:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # Add a check to ensure embedding_vector has the expected number of features\n",
        "            if len(embedding_vector) == n_features:\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "            # else: the embedding_matrix[i] will remain zeros, treating it as an OOV\n",
        "        # OOV words (or words with mismatched dimensions) remain as zeros\n",
        "\n",
        "# --- Preprocessing Data for Model Training ---\n",
        "# Convert text to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_data['Title'])\n",
        "val_sequences = tokenizer.texts_to_sequences(val_data['Title'])\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data['Title'])\n",
        "\n",
        "# Pad sequences\n",
        "train_padded = sequence.pad_sequences(train_sequences, maxlen=sent_length)\n",
        "val_padded = sequence.pad_sequences(val_sequences, maxlen=sent_length)\n",
        "test_padded = sequence.pad_sequences(test_sequences, maxlen=sent_length)\n",
        "\n",
        "# Map labels to numerical values\n",
        "label_mapping = {'uptrend': 0, 'flat': 1, 'downtrend': 2}\n",
        "train_labels = train_data['Label'].map(label_mapping).values\n",
        "val_labels = val_data['Label'].map(label_mapping).values\n",
        "test_labels = test_data['Label'].map(label_mapping).values\n",
        "\n",
        "# Calculate class weights for imbalanced datasets\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "dict_weights = dict(enumerate(class_weights))\n",
        "\n",
        "print(\"--- Step 4: Data Preprocessed and Labels Encoded ---\")\n",
        "print(f\"Shape of train_padded: {train_padded.shape}\")\n",
        "print(f\"Shape of val_padded: {val_padded.shape}\")\n",
        "print(f\"Shape of test_padded: {test_padded.shape}\")\n",
        "print(f\"Sample train_labels (first 10): {train_labels[:10]}\")\n",
        "print(f\"Class weights: {dict_weights}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# --- MODEL ARCHITECTURE ---\n",
        "# Using the Embedding layer as the first layer\n",
        "inputs = Input(shape=(sent_length,))\n",
        "# We pass word indices into an Embedding layer instead of passing raw vectors\n",
        "embed = Embedding(vocab_size, n_features, weights=[embedding_matrix],\n",
        "                  trainable=False)(inputs)\n",
        "\n",
        "# LSTM setup identical to your lab style\n",
        "lstm = LSTM(64, return_sequences=False)\n",
        "outputs_seq = lstm(embed)\n",
        "\n",
        "flat = Flatten()(outputs_seq)\n",
        "outputs = Dense(n_output, activation='softmax')(flat)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "zm2cdmbNEHgY"
      },
      "id": "zm2cdmbNEHgY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training"
      ],
      "metadata": {
        "id": "wZ0wWpiuOD5A"
      },
      "id": "wZ0wWpiuOD5A"
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated fit method for your assignment\n",
        "model.fit(train_padded, to_categorical(train_labels),\n",
        "          validation_data=(val_padded, to_categorical(val_labels)),\n",
        "          epochs=20,\n",
        "          batch_size=4,\n",
        "          class_weight=dict_weights) # Critical for fixing the 0.00 recall"
      ],
      "metadata": {
        "id": "kz_jAqqqEMrR"
      },
      "id": "kz_jAqqqEMrR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix Visualization"
      ],
      "metadata": {
        "id": "k33xuZx2OaAz"
      },
      "id": "k33xuZx2OaAz"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# 1. Get predictions (indices 0, 1, or 2)\n",
        "y_pred_probs = model.predict(test_padded)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# 2. Create Confusion Matrix\n",
        "cm = confusion_matrix(test_labels, y_pred)\n",
        "classes = ['uptrend', 'flat', 'downtrend']\n",
        "\n",
        "# 3. Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=classes, yticklabels=classes)\n",
        "plt.title('Stock Trend Prediction: Actual vs Predicted')\n",
        "plt.ylabel('Actual Trend')\n",
        "plt.xlabel('Predicted Trend')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hBCdWuggEPqG"
      },
      "id": "hBCdWuggEPqG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance Evaluation"
      ],
      "metadata": {
        "id": "245dQnJFOW1Q"
      },
      "id": "245dQnJFOW1Q"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# 1. Get the predicted probabilities\n",
        "y_pred_probs = model.predict(test_padded)\n",
        "\n",
        "# 2. Convert probabilities to class indices (0, 1, or 2)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# 3. Calculate and Print Metrics (Requirement 2.6)\n",
        "# Ensure test_labels are the integer-encoded versions of 'uptrend', 'flat', 'downtrend'\n",
        "print(\"--- Evaluation Metrics ---\")\n",
        "print(classification_report(test_labels, y_pred, target_names=['uptrend', 'flat', 'downtrend']))\n",
        "\n",
        "# Print 10 lines of predictions as required by substep instructions\n",
        "print(\"\\n--- Top 10 Test Predictions ---\")\n",
        "for i in range(10):\n",
        "    print(f\"Actual: {test_labels[i]}, Predicted: {y_pred[i]}\")"
      ],
      "metadata": {
        "id": "Ced0rlAlEhkq"
      },
      "id": "Ced0rlAlEhkq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a more detailed Top 10 for the report\n",
        "results_df = pd.DataFrame({\n",
        "    'News Headline': test_data['Title'].iloc[:10].values,\n",
        "    'Actual Label': [classes[i] for i in test_labels[:10]],\n",
        "    'Predicted Label': [classes[i] for i in y_pred[:10]]\n",
        "})\n",
        "\n",
        "print(\"--- Sample Predictions ---\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "vj1thrJeEm1e"
      },
      "id": "vj1thrJeEm1e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK 3"
      ],
      "metadata": {
        "id": "c7_0DaHLOMZm"
      },
      "id": "c7_0DaHLOMZm"
    },
    {
      "cell_type": "code",
      "source": [
        "# If running on Google Colab, uncomment these:\n",
        "!pip -q install transformers datasets accelerate scikit-learn pandas\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    set_seed\n",
        ")\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "HHr0hLTnEoSH"
      },
      "id": "HHr0hLTnEoSH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize with a pretrained Transformer tokenizer"
      ],
      "metadata": {
        "id": "mfGRAN3OMPmc"
      },
      "id": "mfGRAN3OMPmc"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "model_ckpt = \"distilbert-base-uncased\"  # lightweight, good baseline\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_df[[\"text\", \"label\"]])\n",
        "val_ds   = Dataset.from_pandas(val_df[[\"text\", \"label\"]])\n",
        "test_ds  = Dataset.from_pandas(test_df[[\"text\", \"label\"]])\n",
        "\n",
        "counts = train_df[\"label\"].value_counts().sort_index()\n",
        "print(\"Train label counts:\", counts.to_dict())\n",
        "\n",
        "# Inverse-frequency weights (common baseline)\n",
        "weights = 1.0 / counts.values\n",
        "weights = weights / weights.sum() * len(counts)   # normalize (optional but nice)\n",
        "class_weights = torch.tensor(weights, dtype=torch.float)\n",
        "\n",
        "print(\"Class weights (downtrend, flat, uptrend):\", class_weights.tolist())\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True, max_length = 128)\n",
        "\n",
        "train_ds = train_ds.map(tokenize, batched=True)\n",
        "val_ds   = val_ds.map(tokenize, batched=True)\n",
        "test_ds  = test_ds.map(tokenize, batched=True)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "train_ds\n"
      ],
      "metadata": {
        "id": "LvigWuPcEodU"
      },
      "id": "LvigWuPcEodU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define model + metrics"
      ],
      "metadata": {
        "id": "cKM1e1HLMYtB"
      },
      "id": "cKM1e1HLMYtB"
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_ckpt,\n",
        "    num_labels=3,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"precision_macro\": p,\n",
        "        \"recall_macro\": r,\n",
        "        \"f1_macro\": f1\n",
        "    }\n"
      ],
      "metadata": {
        "id": "bc68-UbPMfS4"
      },
      "id": "bc68-UbPMfS4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Fine-tune (training + validation)"
      ],
      "metadata": {
        "id": "_auUARKuMkSq"
      },
      "id": "_auUARKuMkSq"
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"transformer_stock_trend\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    logging_steps=200,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers import Trainer\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=class_weights.to(logits.device))\n",
        "        loss = loss_fct(logits, labels)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer,            # you can keep it; warning is fine\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "dhdB9YOcJTtt"
      },
      "id": "dhdB9YOcJTtt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate on validation + test"
      ],
      "metadata": {
        "id": "74PxSBJxMsCO"
      },
      "id": "74PxSBJxMsCO"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best checkpoint metrics on VALIDATION:\")\n",
        "val_metrics = trainer.evaluate(eval_dataset=val_ds)\n",
        "val_metrics"
      ],
      "metadata": {
        "id": "7GGcfr4PJfWq"
      },
      "id": "7GGcfr4PJfWq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on TEST\n",
        "test_pred = trainer.predict(test_ds)\n",
        "test_logits = test_pred.predictions\n",
        "test_labels = test_pred.label_ids\n",
        "test_preds = np.argmax(test_logits, axis=-1)\n",
        "\n",
        "print(\"TEST metrics (macro):\")\n",
        "acc = accuracy_score(test_labels, test_preds)\n",
        "p, r, f1, _ = precision_recall_fscore_support(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "print({\"accuracy\": acc, \"precision_macro\": p, \"recall_macro\": r, \"f1_macro\": f1})\n",
        "\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(test_labels, test_preds, target_names=[id2label[i] for i in sorted(id2label.keys())], zero_division=0))\n",
        "\n",
        "print(\"Confusion matrix (rows=true, cols=pred):\")\n",
        "confusion_matrix(test_labels, test_preds)"
      ],
      "metadata": {
        "id": "h-kkQD2wJht1"
      },
      "id": "h-kkQD2wJht1",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}